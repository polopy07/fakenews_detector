import os
# 1. í´ëŸ­ ì œí•œ (ì„ íƒ ì‚¬í•­)
os.system("nvidia-smi -lgc 1000,1600")
import json
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    TrainingArguments, 
    Trainer, 
    TrainerCallback
)
from sklearn.model_selection import train_test_split
import torch
import numpy as np
from collections import defaultdict
from tqdm import tqdm
import nltk
from nltk.tokenize import sent_tokenize
import copy

# 2. JSONL íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
file_path = "news_balanced_50k.jsonl"
with open(file_path, "r", encoding="utf-8") as f:
    data = [json.loads(line) for line in f]

# 3. train/eval split
train_data, eval_data = train_test_split(data, test_size=0.1, stratify=[item["label"] for item in data], random_state=42)
dataset = DatasetDict({
    "train": Dataset.from_list(train_data),
    "eval": Dataset.from_list(eval_data)
})

# 4. í† í¬ë‚˜ì´ì €
tokenizer = AutoTokenizer.from_pretrained("klue/roberta-base", trust_remote_code=True)

# í•œêµ­ì–´ ë¬¸ì¥ í† í°í™”ê°€ ì˜ ì•ˆ ë  ê²½ìš°, ì•„ë˜ì²˜ëŸ¼ ê°„ë‹¨í•œ ê·œì¹™ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥
def korean_sent_tokenize(text):
    return [s.strip() for s in text.strip().replace("?", ".").replace("!", ".").split(".") if s.strip()]

# âœ… ë¬¸ì¥ ê¸°ë°˜ ì²­í¬ ìƒì„± í•¨ìˆ˜
def sentence_window_tokenize(text, max_tokens=512, min_tokens=50):
    sentences = korean_sent_tokenize(text)
    
    chunks = []
    current_chunk = ""
    
    for sent in sentences:
        tentative = current_chunk + sent + " "
        tokenized = tokenizer(tentative, truncation=False)
        length = len(tokenized["input_ids"])

        if length <= max_tokens:
            current_chunk = tentative
        else:
            if current_chunk:
                tokenized_current = tokenizer(current_chunk, truncation=False)
                if len(tokenized_current["input_ids"]) >= min_tokens:
                    chunks.append(current_chunk.strip())
            current_chunk = sent + " "

    if current_chunk:
        tokenized_final = tokenizer(current_chunk, truncation=False)
        if len(tokenized_final["input_ids"]) >= min_tokens:
            chunks.append(current_chunk.strip())
    
    return chunks


# âœ… ìˆ˜ì •ëœ í† í°í™” ë° í‰íƒ„í™” í•¨ìˆ˜
def tokenize_and_flatten(dataset_split):
    new_examples = {"input_ids": [], "attention_mask": [], "labels": [], "article_id": []}
    article_id_counter = 0

    for example in tqdm(dataset_split, desc="Tokenizing"):
        sentence_chunks = sentence_window_tokenize(example['text'], max_tokens=512)

        for chunk_text in sentence_chunks:
            tokens = tokenizer(chunk_text, padding='max_length', truncation=True, max_length=512)
            new_examples["input_ids"].append(tokens["input_ids"])
            new_examples["attention_mask"].append(tokens["attention_mask"])
            new_examples["labels"].append(example["label"])
            new_examples["article_id"].append(article_id_counter)

        article_id_counter += 1

    return Dataset.from_dict(new_examples)

# 7. í† í°í™” ì ìš©
tokenized_dataset = DatasetDict({
    "train": tokenize_and_flatten(dataset["train"]),
    "eval": tokenize_and_flatten(dataset["eval"])
})

# âœ… ëª¨ë¸ ì •ì˜
model = AutoModelForSequenceClassification.from_pretrained("klue/roberta-base", num_labels=2, trust_remote_code=True)

# âœ… í•™ìŠµ ì¸ì ì„¤ì •
training_args = TrainingArguments(
    output_dir="./results_weights_v3",
    eval_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=4,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    learning_rate=2e-5,
    logging_dir="./logs",
    load_best_model_at_end=False,  # chunk_accuracy ê¸°ì¤€ ì €ì¥ ë¹„í™œì„±í™”
    metric_for_best_model=None,    # chunk ê¸°ì¤€ ì €ì¥ë„ ë”
    logging_steps=50,
    fp16=False,
    gradient_accumulation_steps=2,
    no_cuda=False
)

# âœ… chunk ë‹¨ìœ„ í‰ê°€ í•¨ìˆ˜ (ê¸°ë³¸ ìš©ë„)
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    accuracy = (predictions == labels).mean()
    return {"chunk_accuracy": accuracy}

# âœ… ì½œë°± í´ë˜ìŠ¤: ê¸°ì‚¬ ë‹¨ìœ„ ì •í™•ë„ ê¸°ì¤€ ì €ì¥
class SaveBestArticleModelCallback(TrainerCallback):
    def __init__(self, eval_dataset, save_path="best_article_model.pth"):
        self.eval_dataset = eval_dataset
        self.best_accuracy = 0.0
        self.save_path = save_path

    def on_evaluate(self, args, state, control, **kwargs):
        trainer = kwargs['trainer']
        model = trainer.model

        raw_preds = trainer.predict(self.eval_dataset)
        logits = torch.tensor(raw_preds.predictions)
        labels = raw_preds.label_ids
        article_ids = self.eval_dataset["article_id"]

        article_logits = defaultdict(list)
        article_labels = {}
        for i in range(len(article_ids)):
            aid = article_ids[i]
            article_logits[aid].append(logits[i])
            article_labels[aid] = labels[i]

        correct = 0
        for aid, chunk_logits in article_logits.items():
            probs = torch.nn.functional.softmax(torch.stack(chunk_logits), dim=-1)
            weights = torch.linspace(1.0, 2.0, len(probs)).unsqueeze(1)
            weighted_probs = probs * weights
            avg_probs = weighted_probs.mean(dim=0)
            pred = torch.argmax(avg_probs).item()
            if pred == article_labels[aid]:
                correct += 1

        article_accuracy = correct / len(article_labels)
        print(f"âœ… [ì½œë°±] ê¸°ì‚¬ ë‹¨ìœ„ ì •í™•ë„: {article_accuracy:.4f}")

        if article_accuracy > self.best_accuracy:
            self.best_accuracy = article_accuracy
            torch.save(copy.deepcopy(model.state_dict()), self.save_path)
            print(f"ğŸ“¦ ìƒˆ ìµœê³  ê¸°ì‚¬ ì •í™•ë„ ëª¨ë¸ ì €ì¥ë¨: {self.save_path}")

# âœ… Trainer ì •ì˜ (ì½œë°± í¬í•¨)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["eval"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=[SaveBestArticleModelCallback(tokenized_dataset["eval"], save_path="best_article_model.pth")]
)

# âœ… ì´ì–´ì„œ í•™ìŠµ ì‹œì‘
print("ğŸ” í•™ìŠµ ì‹œì‘")
trainer.train(resume_from_checkpoint=True)
print("âœ… í•™ìŠµ ì™„ë£Œ")



# 1. í˜„ì¬ ëª¨ë¸ë¡œ ì²­í¬ ë‹¨ìœ„ ì˜ˆì¸¡ ìˆ˜í–‰
raw_preds = trainer.predict(tokenized_dataset["eval"])
logits = torch.tensor(raw_preds.predictions)
labels = raw_preds.label_ids
article_ids = tokenized_dataset["eval"]["article_id"]

# 2. ê¸°ì‚¬ ë‹¨ìœ„ ì •í™•ë„ ê³„ì‚°
article_logits = defaultdict(list)
article_labels = {}

for i in range(len(article_ids)):
    aid = article_ids[i]
    article_logits[aid].append(logits[i])
    article_labels[aid] = labels[i]

correct = 0
for aid, chunk_logits in article_logits.items():
    probs = torch.nn.functional.softmax(torch.stack(chunk_logits), dim=-1)
    weights = torch.linspace(1.0, 2.0, len(probs)).unsqueeze(1)
    weighted_probs = probs * weights
    avg_probs = weighted_probs.mean(dim=0)
    pred = torch.argmax(avg_probs).item()
    if pred == article_labels[aid]:
        correct += 1

article_accuracy = correct / len(article_labels)
print(f"âœ… ê¸°ì‚¬ ë‹¨ìœ„ ì •í™•ë„: {article_accuracy:.4f}")

# 3. ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
BEST_MODEL_DIR = "./best_article_model"

# 4. ê¸°ì‚¬ ë‹¨ìœ„ ì •í™•ë„ê°€ ê¸°ì¡´ë³´ë‹¤ ë†’ìœ¼ë©´ ëª¨ë¸ ì €ì¥
if not os.path.exists(BEST_MODEL_DIR):
    os.makedirs(BEST_MODEL_DIR)
    trainer.save_model(BEST_MODEL_DIR)
    print(f"ğŸ“¦ ê¸°ì‚¬ ë‹¨ìœ„ ê¸°ì¤€ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥ (ì²˜ìŒ ì €ì¥): {BEST_MODEL_DIR}")

else:
    # ì´ë¯¸ ì €ì¥ëœ ëª¨ë¸ì´ ìˆë‹¤ë©´ ë¹„êµí•´ì„œ ë®ì–´ì“¸ì§€ ê²°ì •
    # ì´ ê²½ìš°, ì •í™•ë„ ê¸°ë¡ì„ ì €ì¥í•œ íŒŒì¼ì´ í•„ìš”í•¨ (ì˜ˆ: best_article_accuracy.txt)
    acc_file = os.path.join(BEST_MODEL_DIR, "best_article_accuracy.txt")
    prev_best = 0.0
    if os.path.exists(acc_file):
        with open(acc_file, "r") as f:
            prev_best = float(f.read().strip())

    if article_accuracy > prev_best:
        trainer.save_model(BEST_MODEL_DIR)
        with open(acc_file, "w") as f:
            f.write(str(article_accuracy))
        print(f"âœ… ê¸°ì‚¬ ë‹¨ìœ„ ê¸°ì¤€ìœ¼ë¡œ ê¸°ì¡´ë³´ë‹¤ ì •í™•ë„ê°€ ë†’ì•„ ìƒˆë¡œ ì €ì¥ë¨: {article_accuracy:.4f}")
    else:
        print(f"â„¹ ê¸°ì¡´ ê¸°ì‚¬ ë‹¨ìœ„ ì •í™•ë„({prev_best:.4f})ë³´ë‹¤ ë‚®ì•„ ì €ì¥í•˜ì§€ ì•ŠìŒ.")