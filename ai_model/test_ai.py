from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ
model_path = "./best_model"

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# GPU ìˆìœ¼ë©´ GPUë¡œ ì˜®ê¸°ê¸°
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
def predict_fake_news(text):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512).to(device)
    
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1)
        pred = torch.argmax(probs, dim=1).item()
    
    return {
        "label": pred,
        "confidence": float(probs[0][pred]),
        "result": "ğŸ“° ì§„ì§œ ë‰´ìŠ¤" if pred == 0 else "âš ï¸ ê°€ì§œ ë‰´ìŠ¤"
    }
text = "ì •ë¶€ëŠ” ì˜¤ëŠ˜ ê¸´ê¸‰ì¬ë‚œì§€ì›ê¸ˆì„ ì¶”ê°€ë¡œ ì§€ê¸‰í•˜ê¸°ë¡œ ë°œí‘œí–ˆë‹¤."
result = predict_fake_news(text)
print("ì˜ˆì¸¡ ê²°ê³¼:", result)
