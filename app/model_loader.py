import html
import torch
import re
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.nn.functional import softmax
import os

# í™˜ê²½ë³€ìˆ˜ì—ì„œ ëª¨ë¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
model_name = os.getenv("MODEL_NAME", "olopy/fakenews")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# from_pretrainedì— torch_dtype ì„¤ì •
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, torch_dtype=torch.float32)
model.to(device)
model.eval()

# í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ ê·œì¹™
fake_keywords = [
    "ìŒëª¨ë¡ ", "ì™¸ê³„ì¸", "ì¢€ë¹„", "ë°±ì‹  ì‚¬ë§", "ê¸°ì§€ ê±´ì„¤", "ì •ë¶€ê°€ ìˆ¨ê²¼ë‹¤",
    "ì¡°ì‘ëœ ìë£Œ", "ë¬´ì¡°ê±´ ì£½ëŠ”ë‹¤", "ê·¹ë¹„ ë¬¸ì„œ", "ì„¸ê³„ì •ë¶€", "DNA ë³€í˜•",
    "5G ê°ì—¼", "ë¶ˆë¡œì¥ìƒ", "ì•” ì¹˜ë£Œ", "ë¯¸êµ­ì´ ì§€ì§„ ìœ ë°œ"
]
max_rule_score = len(fake_keywords)

def korean_sent_tokenize(text):
    return [s.strip() for s in text.replace("?", ".").replace("!", ".").split(".") if s.strip()]

def sentence_window_tokenize(text, max_tokens=512, min_tokens=50):
    sentences = korean_sent_tokenize(text)
    chunks, current_chunk = [], ""
    for sent in sentences:
        tentative = current_chunk + sent + " "
        if len(tokenizer(tentative, truncation=False)["input_ids"]) <= max_tokens:
            current_chunk = tentative
        else:
            if len(tokenizer(current_chunk)["input_ids"]) >= min_tokens:
                chunks.append(current_chunk.strip())
            current_chunk = sent + " "
    if len(tokenizer(current_chunk)["input_ids"]) >= min_tokens:
        chunks.append(current_chunk.strip())
    return chunks

def rule_based_score(text):
    return sum(1 for kw in fake_keywords if kw in text)


def predict_fake_news(text):
    clean_text = html.unescape(text).replace("\r", " ").replace("\n", " ").strip()

    rule_score = rule_based_score(clean_text)
    rule_score_norm = rule_score / max_rule_score

    chunks = sentence_window_tokenize(clean_text)
    chunk_logits = []
    for chunk in chunks:
        inputs = tokenizer(chunk, return_tensors="pt", truncation=True, padding="max_length", max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            logits = model(**inputs).logits
            chunk_logits.append(logits.squeeze(0))

    if not chunk_logits:
        return {"error": "ì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    logits = torch.stack(chunk_logits)
    weights = torch.linspace(1.0, 2.0, len(logits)).unsqueeze(1).to(device)
    weighted_logits = logits * weights
    avg_logits = weighted_logits.mean(dim=0)
    avg_probs = softmax(avg_logits, dim=0)

    real_prob = float(avg_probs[0])
    fake_prob = float(avg_probs[1])
    final_score = 0.7 * fake_prob + 0.3 * rule_score_norm
    label = 1 if final_score >= 0.5 else 0

    result_msg = (
        "ğŸ”´ ê°€ì§œ ë‰´ìŠ¤ë¡œ íŒë‹¨ë¨ (ë”¥ëŸ¬ë‹ + í‚¤ì›Œë“œ ì¼ì¹˜)" if label == 1 and rule_score >= 1 else
        "ğŸ”´ ê°€ì§œ ë‰´ìŠ¤ë¡œ íŒë‹¨ë¨ (í™•ë¥  ê¸°ë°˜)" if label == 1 else
        "âš ï¸ íŒë‹¨ ìœ ë³´" if final_score >= 0.4 else
        "ğŸŸ¢ ì§„ì§œ ë‰´ìŠ¤ë¡œ íŒë‹¨ë¨"
    )

    print("[DEBUG] ìµœì¢… label:", label, "| result:", result_msg)

    return {
        "label": label,
        "confidence": round(final_score, 4),
        "result": result_msg,
        "probabilities": {
            "real": round(real_prob, 4),
            "fake": round(fake_prob, 4)
        },
        "rule_score": rule_score
    }


