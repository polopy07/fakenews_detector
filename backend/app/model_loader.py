import html
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.nn.functional import softmax

# ëª¨ë¸ ê²½ë¡œ
model_path = "C:/Users/sasha/OneDrive/Desktop/best_article_model"
tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
model = AutoModelForSequenceClassification.from_pretrained(model_path, local_files_only=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# ìœ„í—˜ í‚¤ì›Œë“œ
fake_keywords = [
    "ìŒëª¨ë¡ ", "ì™¸ê³„ì¸", "ì¢€ë¹„", "ë°±ì‹  ì‚¬ë§", "ê¸°ì§€ ê±´ì„¤", "ì •ë¶€ê°€ ìˆ¨ê²¼ë‹¤",
    "ì¡°ì‘ëœ ìë£Œ", "ë¬´ì¡°ê±´ ì£½ëŠ”ë‹¤", "ê·¹ë¹„ ë¬¸ì„œ", "ì„¸ê³„ì •ë¶€", "DNA ë³€í˜•",
    "5G ê°ì—¼", "ë¶ˆë¡œì¥ìƒ", "ë¬¼ í•œ ë°©ìš¸ë¡œ ì•” ì¹˜ë£Œ", "ë¯¸êµ­ì´ ì§€ì§„ ìœ ë°œ"
]
max_rule_score = len(fake_keywords)

# ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°
def korean_sent_tokenize(text):
    return [s.strip() for s in text.strip().replace("?", ".").replace("!", ".").split(".") if s.strip()]

# ì²­í¬ ë¶„í•  í•¨ìˆ˜
def sentence_window_tokenize(text, tokenizer, max_tokens=512, min_tokens=50):
    sentences = korean_sent_tokenize(text)
    chunks = []
    current_chunk = ""

    for sent in sentences:
        tentative = current_chunk + sent + " "
        tokenized = tokenizer(tentative, truncation=False)
        if len(tokenized["input_ids"]) <= max_tokens:
            current_chunk = tentative
        else:
            if len(tokenizer(current_chunk)["input_ids"]) >= min_tokens:
                chunks.append(current_chunk.strip())
            current_chunk = sent + " "

    if len(tokenizer(current_chunk)["input_ids"]) >= min_tokens:
        chunks.append(current_chunk.strip())

    return chunks

# í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜
def rule_based_score(text):
    return sum(1 for kw in fake_keywords if kw in text)

# ì˜ˆì¸¡ í•¨ìˆ˜
def predict_fake_news(text):
    clean_text = html.unescape(text).replace("\r", " ").replace("\n", " ").strip()
    rule_score = rule_based_score(clean_text)
    rule_score_norm = rule_score / max_rule_score

    chunks = sentence_window_tokenize(clean_text, tokenizer)
    chunk_logits = []

    for chunk in chunks:
        inputs = tokenizer(chunk, return_tensors="pt", padding="max_length", truncation=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            output = model(**inputs)
            chunk_logits.append(output.logits.squeeze(0))

    if not chunk_logits:
        return {"error": "ì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì…ë ¥ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ í† í°í™” ì‹¤íŒ¨."}

    # ê¸°ì‚¬ ë‹¨ìœ„ softmax í‰ê·  (ê°€ì¤‘ì¹˜ í¬í•¨)
    logits = torch.stack(chunk_logits)
    weights = torch.linspace(1.0, 2.0, len(logits)).unsqueeze(1).to(device)
    weighted_logits = logits * weights
    avg_logits = weighted_logits.mean(dim=0)
    avg_probs = torch.nn.functional.softmax(avg_logits, dim=0)
    
    real_prob = float(avg_probs[0])
    fake_prob = float(avg_probs[1])

    # ìµœì¢… í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
    final_score = 0.7 * fake_prob + 0.3 * rule_score_norm
    label = 1 if final_score >= 0.5 else 0

    # ë©”ì‹œì§€
    if label == 1 and rule_score >= 1:
        result = "ğŸ”´ ê°€ì§œ ë‰´ìŠ¤ë¡œ íŒë‹¨ë¨ (ë”¥ëŸ¬ë‹ + í‚¤ì›Œë“œ ì¼ì¹˜)"
    elif label == 1:
        result = "ğŸ”´ ê°€ì§œ ë‰´ìŠ¤ë¡œ íŒë‹¨ë¨ (í™•ë¥  ê¸°ë°˜)"
    elif final_score >= 0.4:
        result = "âš ï¸ íŒë‹¨ ìœ ë³´"
    else:
        result = "ğŸŸ¢ ì§„ì§œ ë‰´ìŠ¤ë¡œ íŒë‹¨ë¨"

    print(f"[DEBUG] Softmax: {avg_probs.tolist()} | Rule score: {rule_score} | Hybrid score: {final_score:.4f}")
    print(f"[DEBUG] fake_prob: {fake_prob}, rule_score: {rule_score}, final_score: {final_score}, label: {label}")

    return {
        "label": label,
        "confidence": round(final_score, 4),
        "result": result,
        "probabilities": {
            "real": round(real_prob, 4),
            "fake": round(fake_prob, 4)
        },
        "rule_score": rule_score
    }
