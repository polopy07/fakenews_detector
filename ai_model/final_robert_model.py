import torch
import numpy as np
from datasets import Dataset
from sklearn.metrics import accuracy_score, f1_score
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding

# âœ… 1. ë°ì´í„° ë¡œë“œ (ì˜ˆì‹œë¡œ JSONL íŒŒì¼ì„ ì‚¬ìš©)
import json
with open("/content/fake_news_dataset_all_rewritten.jsonl", "r", encoding="utf-8") as f:
    data_list = [json.loads(line) for line in f]

# í…ìŠ¤íŠ¸ì™€ ë¼ë²¨ ì¶”ì¶œ
texts = [item["text"] for item in data_list]
labels = [int(item["label"]) for item in data_list]  # ë¼ë²¨ì€ ë°˜ë“œì‹œ intë¡œ ë³€í™˜

# âœ… 2. Dataset ìƒì„±
dataset = Dataset.from_dict({"text": texts, "label": labels})
dataset = dataset.train_test_split(test_size=0.2, seed=42)
train_dataset = dataset["train"]
test_dataset = dataset["test"]

# âœ… 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "roberta-base"
tokenizer = RobertaTokenizer.from_pretrained(model_name)
model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)

# âœ… 4. í† í°í™” í•¨ìˆ˜ ì •ì˜
def tokenize(batch):
    return tokenizer(batch["text"], padding=True, truncation=True, max_length=256)

train_dataset = train_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)

# âœ… 5. í‰ê°€ í•¨ìˆ˜ ì •ì˜
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds, average="binary", pos_label=1)
    }

# âœ… 6. í›ˆë ¨ ì¸ì ì„¤ì •
training_args = TrainingArguments(
    output_dir="./roberta_fake_news_model",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_dir='./logs',
    logging_steps=50,
    seed=42,
    report_to="none"  # WandB ë¹„í™œì„±í™”
)

# âœ… 7. Trainer ìƒì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    data_collator=DataCollatorWithPadding(tokenizer)
)

# âœ… 8. ëª¨ë¸ í•™ìŠµ
print("ğŸš€ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
trainer.train()
print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

# âœ… 9. ëª¨ë¸ ì €ì¥
model_dir = "./final_roberta_model"
trainer.save_model(model_dir)
tokenizer.save_pretrained(model_dir)

# âœ… 10. ëª¨ë¸ zip ì••ì¶•
import shutil
shutil.make_archive("final_roberta_model", 'zip', model_dir)
print("âœ… ëª¨ë¸ì´ 'final_roberta_model.zip'ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. VSCode ë“±ì—ì„œ ë‹¤ìš´ë¡œë“œí•´ ì‚¬ìš©í•˜ì„¸ìš”.")
