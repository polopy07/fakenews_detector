import pandas as pd
import json
import re
import multiprocessing as mp
from bareunpy import Tagger
from tqdm import tqdm  # ì§„í–‰ë¥  í‘œì‹œ ë¼ì´ë¸ŒëŸ¬ë¦¬

API_KEY = "https://bareun.ai"
file_path = '.json'

# JSON íŒŒì¼ ë¡œë“œ
with open(file_path, "r", encoding="utf-8") as file:
    json_data = json.load(file)

def flatten_json(json_obj, parent_key='', sep='_'):
    flattened_dict = {}
    for key, value in json_obj.items():
        new_key = f"{parent_key}{sep}{key}" if parent_key else key
        if isinstance(value, dict):
            flattened_dict.update(flatten_json(value, new_key, sep))
        elif isinstance(value, list):
            if all(isinstance(i, dict) for i in value):
                for i, item in enumerate(value):
                    flattened_dict.update(flatten_json(item, f"{new_key}_{i}", sep))
            else:
                flattened_dict[new_key] = " ".join(map(str, value))
        else:
            flattened_dict[new_key] = value
    return flattened_dict

# JSON ë°ì´í„° í‰íƒ„í™” í›„ DataFrame ë³€í™˜
data = json_data['data']
flattened_data = [flatten_json(doc) for doc in data]
df = pd.DataFrame(flattened_data)

# ì¤‘ë³µ ì œê±°
df = df.drop_duplicates(subset=['doc_id', 'doc_title', 'doc_source', 
                                'doc_published', 'doc_class_code', 
                                'created', 'paragraphs_0_context'])

print(df.info())  # ë°ì´í„° í™•ì¸

# âœ… í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
def clean_text(text):
    if pd.isna(text):
        return ""
    text = re.sub(r"<[^>]*>", "", text)  
    text = re.sub(r"[^\w\sã„±-ã…ê°€-í£]", "", text)  
    text = re.sub(r"\d+", "", text)  
    text = text.lower()  
    return text.strip()

# ì „ì²˜ë¦¬ ì ìš©
df["doc_title"] = df["doc_title"].apply(clean_text)
df["paragraphs_0_context"] = df["paragraphs_0_context"].apply(clean_text)

# âœ… ë³‘ë ¬ ì‹¤í–‰ì„ ìœ„í•œ í•¨ìˆ˜
def tokenize_and_stem(text):
    if pd.isna(text):
        return ""
    
    # ğŸ”¥ ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ `Tagger` ìƒˆë¡œ ìƒì„±
    local_tagger = Tagger(API_KEY, 'localhost')
    
    tokens = local_tagger.morphs(text)
    return " ".join(tokens)

# âœ… ë©€í‹°í”„ë¡œì„¸ì‹± + ì§„í–‰ë¥  ì¶œë ¥
def process_with_multiprocessing(df, column_name):
    num_workers = max(1, mp.cpu_count() // 2)  # CPU ì ˆë°˜ë§Œ ì‚¬ìš©
    with mp.get_context("spawn").Pool(num_workers) as pool:
        # ğŸ”¥ tqdm ì¶”ê°€: ì§„í–‰ë¥  ì¶œë ¥
        results = list(tqdm(pool.imap(tokenize_and_stem, df[column_name]), total=len(df), desc=f"Processing {column_name}"))
        df[column_name] = results  # ë³€í™˜ëœ ë°ì´í„° ì €ì¥
    return df

if __name__ == "__main__":
    df = process_with_multiprocessing(df, "doc_title")
    df = process_with_multiprocessing(df, "paragraphs_0_context")

    print(df.head())  

    # âœ… JSON ì €ì¥
    save_path_json = "morphs_news_data.json"
    df.to_json(save_path_json, orient="records", force_ascii=False)

    print(f"íŒŒì¼ì´ ì €ì¥ë¨: {save_path_json}")